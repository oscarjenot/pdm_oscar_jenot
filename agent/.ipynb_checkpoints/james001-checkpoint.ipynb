{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cultural-privilege",
   "metadata": {},
   "source": [
    "# crane-v0 - Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "irish-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "exact-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('crane-v0')\n",
    "env.seed(1); torch.manual_seed(1); np.random.seed(1)\n",
    "#PATH = glob.glob(os.path.expanduser('~/tboardlogs/'))[0]\n",
    "PATH = glob.glob(os.path.expanduser('~/tboardlogs/'))\n",
    "writer = SummaryWriter('{}{}'.format(PATH, datetime.now().strftime('%b%d_%H-%M-%S')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-crossing",
   "metadata": {},
   "source": [
    "Running the environment with random actions produces no successful episodes in a run of 1000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_position = -.4\n",
    "positions = np.ndarray([-2.4,2.4])\n",
    "rewards = []\n",
    "successful = []\n",
    "for episode in range(1000):\n",
    "    running_reward = 0\n",
    "    env.reset()\n",
    "    done = False\n",
    "    for i in range(200):\n",
    "        state, reward, done, _ = env.step(np.random.randint(0,3))\n",
    "        # Give a reward for reaching a new maximum position\n",
    "        if state[0] > max_position:\n",
    "            max_position = state[0]\n",
    "            positions = np.append(positions, [[episode, max_position]], axis=0)\n",
    "            running_reward += 10\n",
    "        else:\n",
    "            running_reward += reward\n",
    "        if done: \n",
    "            if state[0] >= 0.5:\n",
    "                successful.append(episode)\n",
    "            rewards.append(running_reward)\n",
    "            break\n",
    "\n",
    "print('Furthest Position: {}'.format(max_position))\n",
    "plt.figure(1, figsize=[10,5])\n",
    "plt.subplot(211)\n",
    "plt.plot(positions[:,0], positions[:,1])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Furthest Position')\n",
    "plt.subplot(212)\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()\n",
    "print('successful episodes: {}'.format(np.count_nonzero(successful)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
